@ qkv unpad shape:  torch.Size([64884, 3, 16, 64])
@ qkv shape:  torch.Size([64, 1024, 3, 16, 64])
FlashAttention - Forward pass
<torch.utils.benchmark.utils.common.Measurement object at 0x7f84051909a0>
fn_amp(*inputs, **kwinputs)
  8.41 ms
  1 measurement, 30 runs , 48 threads
PyTorch Standard Attention - Forward pass
<torch.utils.benchmark.utils.common.Measurement object at 0x7f84051909d0>
fn_amp(*inputs, **kwinputs)
  43.99 ms
  1 measurement, 30 runs , 48 threads
